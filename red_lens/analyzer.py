# -*- coding: utf-8 -*-
"""
Analyzer module for RedLens
Identifies viral content (çˆ†æ¬¾) and provides AI insights
"""

import os
import sys
import requests
from pathlib import Path
from typing import List, Dict, Any, Optional
from datetime import datetime

# Add parent directory to path
MEDIA_CRAWLER_ROOT = Path(__file__).parent.parent
sys.path.insert(0, str(MEDIA_CRAWLER_ROOT))

from red_lens.db import BloggerDB, NoteDB, init_db


# Directory for storing cover images
COVER_DIR = Path(__file__).parent / "assets" / "covers"
COVER_DIR.mkdir(parents=True, exist_ok=True)

# Directory for storing AI reports
REPORTS_DIR = Path(__file__).parent / "reports"
REPORTS_DIR.mkdir(parents=True, exist_ok=True)


def calculate_outlier_threshold(user_id: str, multiplier: float = 3.0, min_likes: int = 500) -> float:
    """
    Calculate the outlier threshold for a user's notes

    Args:
        user_id: User ID
        multiplier: How many times above average to consider outlier (default: 3x)
        min_likes: Minimum absolute likes to consider outlier (default: 500)

    Returns:
        Outlier threshold value
    """
    avg_likes = NoteDB.get_avg_likes_by_user(user_id)
    threshold = max(avg_likes * multiplier, min_likes)
    return threshold


def identify_outliers(user_id: str, multiplier: float = 3.0, min_likes: int = 500) -> List[Dict[str, Any]]:
    """
    Identify viral notes (çˆ†æ¬¾) for a specific blogger

    A note is considered an outlier if:
    - likes > avg_likes * multiplier (default: 3x)
    - AND likes > min_likes (default: 500)

    Args:
        user_id: User ID
        multiplier: Outlier multiplier
        min_likes: Minimum absolute likes

    Returns:
        List of outlier notes
    """
    notes = NoteDB.get_notes_by_user(user_id)

    if not notes:
        return []

    threshold = calculate_outlier_threshold(user_id, multiplier, min_likes)
    outliers = []

    for note in notes:
        likes = note["likes"]
        if likes >= threshold:
            outliers.append(note)
            # Mark as outlier in database
            NoteDB.update_outlier_status(note["note_id"], True)

    return outliers


def analyze_blogger(user_id: str) -> Dict[str, Any]:
    """
    Comprehensive analysis of a blogger's content

    Args:
        user_id: User ID

    Returns:
        Analysis results dictionary
    """
    blogger = BloggerDB.get_blogger(user_id)
    if not blogger:
        return {"error": "Blogger not found"}

    notes = NoteDB.get_notes_by_user(user_id)
    if not notes:
        return {
            "error": "No notes found",
            "blogger": blogger
        }

    # Calculate metrics
    total_notes = len(notes)
    avg_likes = NoteDB.get_avg_likes_by_user(user_id)
    total_likes = sum(note["likes"] for note in notes)
    total_collects = sum(note["collects"] for note in notes)
    total_comments = sum(note["comments"] for note in notes)

    # Find outliers
    outliers = identify_outliers(user_id)
    outlier_rate = len(outliers) / total_notes if total_notes > 0 else 0

    # Content type distribution
    video_count = sum(1 for note in notes if note["type"] == "video")
    image_count = sum(1 for note in notes if note["type"] == "image")

    # Engagement rate (likes + collects + comments) / notes
    total_engagement = total_likes + total_collects + total_comments
    avg_engagement = total_engagement / total_notes if total_notes > 0 else 0

    analysis = {
        "blogger": blogger,
        "total_notes": total_notes,
        "avg_likes": avg_likes,
        "total_likes": total_likes,
        "total_collects": total_collects,
        "total_comments": total_comments,
        "total_engagement": total_engagement,
        "avg_engagement": avg_engagement,
        "outlier_count": len(outliers),
        "outlier_rate": outlier_rate,
        "video_count": video_count,
        "image_count": image_count,
        "outliers": outliers
    }

    return analysis


def download_cover_image(note_id: str, cover_url: str, overwrite: bool = False) -> Optional[str]:
    """
    Download cover image for a note

    Args:
        note_id: Note ID
        cover_url: URL of the cover image
        overwrite: Whether to overwrite existing file

    Returns:
        Local file path if successful, None otherwise
    """
    if not cover_url:
        print(f"  âš  No cover URL for note {note_id}")
        return None

    # Determine file extension from URL
    ext = ".jpg"  # default
    if ".png" in cover_url.lower():
        ext = ".png"
    elif ".webp" in cover_url.lower():
        ext = ".webp"

    local_path = COVER_DIR / f"{note_id}{ext}"

    # Check if already exists
    if local_path.exists() and not overwrite:
        print(f"  âœ“ Cover already exists: {local_path.name}")
        return str(local_path)

    try:
        # Download image
        response = requests.get(cover_url, timeout=30)
        response.raise_for_status()

        # Save to file
        with open(local_path, 'wb') as f:
            f.write(response.content)

        print(f"  âœ“ Downloaded cover: {local_path.name}")
        return str(local_path)

    except Exception as e:
        print(f"  âœ— Failed to download cover for {note_id}: {e}")
        return None


def download_outlier_covers(user_id: Optional[str] = None, overwrite: bool = False) -> int:
    """
    Download cover images for all outlier notes

    Args:
        user_id: Optional user ID to filter (if None, download for all users)
        overwrite: Whether to overwrite existing files

    Returns:
        Number of covers successfully downloaded
    """
    print(f"\n{'='*60}")
    print(f"RedLens Cover Downloader")
    print(f"{'='*60}")
    print(f"Target: {'All users' if not user_id else f'User {user_id}'}")
    print(f"Save directory: {COVER_DIR}")
    print(f"{'='*60}\n")

    outlier_notes = NoteDB.get_outlier_notes(user_id=user_id)

    if not outlier_notes:
        print("âœ— No outlier notes found")
        return 0

    print(f"ðŸ“¥ Found {len(outlier_notes)} outlier note(s) to download")

    downloaded = 0
    for note in outlier_notes:
        note_id = note["note_id"]
        cover_url = note["cover_url"]

        print(f"\nðŸ“· {note['title'][:30]}... (Likes: {note['likes']:,})")

        local_path = download_cover_image(note_id, cover_url, overwrite=overwrite)

        if local_path:
            # Update database with local path
            NoteDB.update_local_cover_path(note_id, local_path)
            downloaded += 1

    print(f"\n{'='*60}")
    print(f"âœ“ Download completed!")
    print(f"  Total outliers: {len(outlier_notes)}")
    print(f"  Successfully downloaded: {downloaded}")
    print(f"  Failed: {len(outlier_notes) - downloaded}")
    print(f"{'='*60}\n")

    return downloaded


def get_report_file_path(user_id: str) -> Path:
    """
    Get the file path for a user's AI report

    Args:
        user_id: User ID

    Returns:
        Path object for the report file
    """
    return REPORTS_DIR / f"{user_id}_report.md"


def save_report_to_file(user_id: str, report: str) -> bool:
    """
    Save AI report to file

    Args:
        user_id: User ID
        report: Report content

    Returns:
        True if successful, False otherwise
    """
    try:
        report_file = get_report_file_path(user_id)
        with open(report_file, 'w', encoding='utf-8') as f:
            f.write(report)
        print(f"  âœ“ Report saved to: {report_file.name}")
        return True
    except Exception as e:
        print(f"  âœ— Failed to save report: {e}")
        return False


def load_report_from_file(user_id: str) -> Optional[str]:
    """
    Load AI report from file

    Args:
        user_id: User ID

    Returns:
        Report content if exists, None otherwise
    """
    try:
        report_file = get_report_file_path(user_id)
        if report_file.exists():
            with open(report_file, 'r', encoding='utf-8') as f:
                return f.read()
        return None
    except Exception as e:
        print(f"  âœ— Failed to load report: {e}")
        return None


def report_exists(user_id: str) -> bool:
    """
    Check if AI report exists for a user

    Args:
        user_id: User ID

    Returns:
        True if report exists, False otherwise
    """
    return get_report_file_path(user_id).exists()


def delete_report_file(user_id: str) -> bool:
    """
    Delete AI report file for a user

    Args:
        user_id: User ID

    Returns:
        True if deleted, False otherwise
    """
    try:
        report_file = get_report_file_path(user_id)
        if report_file.exists():
            report_file.unlink()
            print(f"  âœ“ Report deleted: {report_file.name}")
            return True
        return False
    except Exception as e:
        print(f"  âœ— Failed to delete report: {e}")
        return False


def generate_ai_report(user_id: str, use_mock: bool = True, force_regenerate: bool = False) -> str:
    """
    Generate AI insights report for a blogger

    Args:
        user_id: User ID
        use_mock: If True, return mock report. If False, call Deepseek API
        force_regenerate: If True, regenerate report even if file exists

    Returns:
        AI-generated report text
    """
    # Import config here to avoid circular import
    import config

    print(f"\n{'='*60}")
    print(f"RedLens AI Insights")
    print(f"{'='*60}\n")

    analysis = analyze_blogger(user_id)

    if "error" in analysis:
        return f"Error: {analysis['error']}"

    blogger = analysis["blogger"]
    print(f"ðŸ¤– Generating AI insights for: {blogger['nickname']}")

    # Check if report file exists (unless force regenerate)
    if not force_regenerate:
        existing_report = load_report_from_file(user_id)
        if existing_report:
            print("  âœ“ Using existing report from file")
            return existing_report

    if use_mock:
        # Mock AI report for testing
        print("  [Using mock AI report]")

        # Get fans count (current_fans or initial_fans)
        fans_count = blogger.get('current_fans', blogger.get('initial_fans', 0))

        report = f"""
# AI æ´žå¯ŸæŠ¥å‘Šï¼š{blogger['nickname']}

## ðŸ“Š æ•°æ®æ¦‚è§ˆ

- **ç²‰ä¸æ•°**: {fans_count:,}
- **æ€»ç¬”è®°æ•°**: {analysis['total_notes']}
- **å¹³å‡ç‚¹èµž**: {analysis['avg_likes']:.0f}
- **æ€»äº’åŠ¨é‡**: {analysis['total_engagement']:,} (ç‚¹èµž+æ”¶è—+è¯„è®º)
- **çˆ†æ¬¾çŽ‡**: {analysis['outlier_rate']:.1%} ({analysis['outlier_count']}/{analysis['total_notes']})
- **å†…å®¹ç±»åž‹**: å›¾æ–‡ {analysis['image_count']} ç¯‡ | è§†é¢‘ {analysis['video_count']} ç¯‡

## ðŸ”¥ çˆ†æ¬¾åˆ†æž

è¯¥åšä¸»å…±äº§å‡º **{analysis['outlier_count']} ç¯‡çˆ†æ¬¾å†…å®¹**ï¼Œçˆ†æ¬¾çŽ‡è¾¾ {analysis['outlier_rate']:.1%}ã€‚

"""

        if analysis['outliers']:
            report += "### Top çˆ†æ¬¾ç¬”è®°\n\n"
            for i, note in enumerate(sorted(analysis['outliers'], key=lambda x: x['likes'], reverse=True)[:3], 1):
                report += f"{i}. **{note['title'][:40]}...**\n"
                report += f"   - ç‚¹èµž: {note['likes']:,} | æ”¶è—: {note['collects']:,} | è¯„è®º: {note['comments']:,}\n"
                report += f"   - ç±»åž‹: {note['type']}\n\n"

        report += """
## ðŸ’¡ AI å»ºè®®

1. **å†…å®¹ç­–ç•¥**: åŸºäºŽçˆ†æ¬¾æ•°æ®ï¼Œè¯¥åšä¸»åœ¨[ä¸»é¢˜]æ–¹é¢è¡¨çŽ°çªå‡ºï¼Œå»ºè®®ç»§ç»­æ·±è€•è¯¥é¢†åŸŸã€‚

2. **å‘å¸ƒèŠ‚å¥**: å¹³å‡äº’åŠ¨é‡è¾ƒé«˜ï¼Œè¯´æ˜Žç²‰ä¸ç²˜æ€§è‰¯å¥½ï¼Œå»ºè®®ä¿æŒç¨³å®šçš„æ›´æ–°é¢‘çŽ‡ã€‚

3. **å†…å®¹å½¢å¼**: å›¾æ–‡/è§†é¢‘å†…å®¹å„æœ‰ä¼˜åŠ¿ï¼Œå»ºè®®æ ¹æ®ä¸»é¢˜ç‰¹ç‚¹é€‰æ‹©åˆé€‚çš„å‘ˆçŽ°æ–¹å¼ã€‚

---
ðŸ¤– Generated by Mock AI | RedLens v1.2.0
"""

        # Save report to file
        save_report_to_file(user_id, report)

        return report

    else:
        # Real Deepseek API call
        print("  [Calling Deepseek API...]")

        try:
            from openai import OpenAI

            # Check API key
            if not config.DEEPSEEK_API_KEY:
                return "Error: DEEPSEEK_API_KEY not configured. Please set the environment variable or configure it in config/ai_config.py"

            # Prepare data for prompt
            notes = NoteDB.get_notes_by_user(user_id)

            # Get top outliers for analysis
            top_outliers = sorted(analysis['outliers'], key=lambda x: x['likes'], reverse=True)[:5]

            # Format top notes info with titles (no cover URLs as Deepseek doesn't support vision)
            top_notes_info = ""
            for i, note in enumerate(top_outliers, 1):
                top_notes_info += f"\n{i}. **{note['title']}**\n"
                top_notes_info += f"   - ç‚¹èµž: {note['likes']:,} | æ”¶è—: {note['collects']:,} | è¯„è®º: {note['comments']:,}\n"
                top_notes_info += f"   - ç±»åž‹: {note['type']}\n"
                top_notes_info += f"   - å‘å¸ƒæ—¶é—´: {note.get('publish_time', 'N/A')}\n"

            # Calculate time distribution
            time_dist = {}
            for note in notes:
                publish_time = note.get('publish_time', '')
                if publish_time:
                    try:
                        hour = int(publish_time.split(':')[0]) if ':' in str(publish_time) else 0
                        time_dist[hour] = time_dist.get(hour, 0) + 1
                    except:
                        pass

            time_distribution = "å‘å¸ƒæ—¶é—´ä¸»è¦é›†ä¸­åœ¨: " + ", ".join([f"{h}æ—¶({c}ç¯‡)" for h, c in sorted(time_dist.items(), key=lambda x: x[1], reverse=True)[:5]])

            # Calculate publish frequency
            last_publish = max([note.get('publish_time', '') for note in notes]) if notes else "N/A"
            publish_frequency = f"çº¦ {len(notes) / 30:.1f} ç¯‡/æœˆ" if len(notes) >= 30 else f"{len(notes)} ç¯‡æ€»è®¡"

            # Build user prompt
            # Fix: Use current_fans (or initial_fans as fallback)
            fans_count = blogger.get('current_fans', blogger.get('initial_fans', 0))

            # Calculate interaction rate (avoid division by zero)
            total_interactions = analysis['avg_likes'] + (analysis['total_collects'] / analysis['total_notes'] if analysis['total_notes'] > 0 else 0) + (analysis['total_comments'] / analysis['total_notes'] if analysis['total_notes'] > 0 else 0)
            interaction_rate = (total_interactions / fans_count * 100) if fans_count > 0 else 0

            user_prompt = config.AI_USER_PROMPT_TEMPLATE.format(
                nickname=blogger['nickname'],
                user_id=user_id,
                fans=fans_count,
                total_notes=analysis['total_notes'],
                avg_likes=analysis['avg_likes'],
                avg_collects=analysis['total_collects'] / analysis['total_notes'] if analysis['total_notes'] > 0 else 0,
                avg_comments=analysis['total_comments'] / analysis['total_notes'] if analysis['total_notes'] > 0 else 0,
                interaction_rate=interaction_rate,
                outlier_rate=analysis['outlier_rate'],
                image_count=analysis['image_count'],
                video_count=analysis['video_count'],
                last_publish_date=last_publish,
                publish_frequency=publish_frequency,
                top_n=len(top_outliers),
                top_notes_info=top_notes_info,
                time_distribution=time_distribution
            )

            # Call Deepseek API
            client = OpenAI(
                api_key=config.DEEPSEEK_API_KEY,
                base_url=config.DEEPSEEK_BASE_URL
            )

            print(f"  â€¢ Model: {config.AI_MODEL}")
            print(f"  â€¢ Max tokens: {config.AI_MAX_TOKENS}")

            response = client.chat.completions.create(
                model=config.AI_MODEL,
                messages=[
                    {"role": "system", "content": config.AI_SYSTEM_PROMPT},
                    {"role": "user", "content": user_prompt}
                ],
                max_tokens=config.AI_MAX_TOKENS,
                temperature=config.AI_TEMPERATURE,
                timeout=config.AI_REQUEST_TIMEOUT
            )

            report_content = response.choices[0].message.content

            # Add header and footer
            report = f"# AI æ´žå¯ŸæŠ¥å‘Šï¼š{blogger['nickname']}\n\n"
            report += report_content
            report += f"\n\n---\nðŸ¤– Generated by Deepseek AI ({config.AI_MODEL}) | RedLens v1.2.0"

            # Save report to file
            save_report_to_file(user_id, report)

            print("  âœ“ AI report generated successfully")

            return report

        except ImportError as e:
            return f"Error: openai package not installed. Run: pip install openai\nDetails: {str(e)}"
        except Exception as e:
            error_msg = f"Error generating AI report: {str(e)}"
            print(f"  âœ— {error_msg}")
            return error_msg


def analyze_all_bloggers() -> List[Dict[str, Any]]:
    """
    Analyze all scraped bloggers and rank by viral rate

    Returns:
        List of analysis results sorted by outlier_rate
    """
    print(f"\n{'='*60}")
    print(f"RedLens Batch Analysis")
    print(f"{'='*60}\n")

    scraped_bloggers = [b for b in BloggerDB.get_all_bloggers() if b["status"] == "scraped"]

    if not scraped_bloggers:
        print("âœ— No scraped bloggers to analyze")
        return []

    print(f"ðŸ“Š Analyzing {len(scraped_bloggers)} blogger(s)...\n")

    analyses = []
    for blogger in scraped_bloggers:
        user_id = blogger["user_id"]
        analysis = analyze_blogger(user_id)

        if "error" not in analysis:
            analyses.append(analysis)

            # Print summary
            print(f"âœ“ {blogger['nickname']}")
            print(f"  â€¢ Notes: {analysis['total_notes']}")
            print(f"  â€¢ Avg likes: {analysis['avg_likes']:.0f}")
            print(f"  â€¢ Outliers: {analysis['outlier_count']} ({analysis['outlier_rate']:.1%})")

    # Sort by outlier rate
    analyses.sort(key=lambda x: x["outlier_rate"], reverse=True)

    print(f"\n{'='*60}")
    print(f"âœ“ Analysis completed!")
    print(f"{'='*60}\n")

    return analyses


def main():
    """Test the analyzer module"""
    init_db()

    print("=" * 60)
    print("STEP 1: Analyze all bloggers")
    print("=" * 60)

    analyses = analyze_all_bloggers()

    if analyses:
        print("\n" + "=" * 60)
        print("STEP 2: Download outlier covers")
        print("=" * 60)

        download_outlier_covers()

        print("\n" + "=" * 60)
        print("STEP 3: Generate AI report for top blogger")
        print("=" * 60)

        # Generate report for the top blogger
        top_blogger = analyses[0]
        user_id = top_blogger["blogger"]["user_id"]
        report = generate_ai_report(user_id, use_mock=True)

        print("\n" + "=" * 60)
        print("AI REPORT")
        print("=" * 60)
        print(report)


if __name__ == "__main__":
    main()
